{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning on MCU - Ex3.2\n",
    "\n",
    "This exercise will introduce you to Keras, one of the most popular frameworks used in Deep Learning.\n",
    "\n",
    "You will train a simple multi-layer perceptron to predict Human Activity from Smartphone Accelerometer and Gyroscope Data.\n",
    "\n",
    "We use a dataset of 3-axial accelerometer signals from an academic experiment on the UC Irvine Machine Learning Repository.\n",
    "\n",
    "The dataset is downloaded in the code snippet below and you can you can also find the description of the dataset [here](https://archive.ics.uci.edu/dataset/341/smartphone+based+recognition+of+human+activities+and+postural+transitions) .\n",
    "\n",
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# If required, download the dataset\n",
    "import requests\n",
    "import os.path\n",
    "import zipfile\n",
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "random.seed(7)\n",
    "\n",
    "if (not os.path.isdir('./HAPT Data Set')):\n",
    "    open('./HAPT Data Set.zip', 'wb').write(requests.get(\n",
    "        \"https://archive.ics.uci.edu/ml/machine-learning-databases/00341/HAPT%20Data%20Set.zip\", \n",
    "        allow_redirects=True).content)\n",
    "    zipfile.ZipFile('./HAPT Data Set.zip', 'r').extractall('./HAPT Data Set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the accelerometer and gyroscope data.\n",
    "We read the feature names from features.txt and the activity labels from activity_labels.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./HAPT Data Set/features.txt') as f:\n",
    "    features = f.read().split()\n",
    "\n",
    "print('There are {} features.'.format(len(features)))\n",
    "    \n",
    "with open('./HAPT Data Set/activity_labels.txt') as f:\n",
    "    activity_labels = f.readlines()\n",
    "\n",
    "activity_df = [x.split() for x in activity_labels]\n",
    "print('There are {} activities.'.format(len(activity_df)))\n",
    "pd.DataFrame(activity_df, columns = ['Activity_id', 'Activity_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are pre-split into training and test sets. Let's load the features x and the labels y, and have a look at a few features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_table('./HAPT Data Set/Train/X_train.txt',\n",
    "             header = None, sep = \" \")\n",
    "X_train.iloc[:10, :10].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_table('./HAPT Data Set/Train/y_train.txt',\n",
    "             header = None, sep = \" \", names = ['Activity_id'])\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = pd.read_table('./HAPT Data Set/Test/X_test.txt',\n",
    "             header = None, sep = \" \")\n",
    "y_test = pd.read_table('./HAPT Data Set/Test/y_test.txt',\n",
    "             header = None, sep = \" \", names = ['Activity_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Human Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import the necessary packages\n",
    "\n",
    "\n",
    "# Note: use Tensor Flow backend for Keras as suggested on keras.io:\n",
    "# \"At this time, we recommend that Keras users who use multi-backend Keras with the TensorFlow backend switch to tf.keras in TensorFlow 2.0. tf.keras is better maintained and has better integration with TensorFlow features (eager execution, distribution support and other).\"\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers, models, regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the accelerometer and gyroscope data.\n",
    "X_train_full = X_train.values\n",
    "X_test_data = X_test.values\n",
    "y_train_full = y_train.values.flatten()\n",
    "y_test_data = y_test.values.flatten()\n",
    "\n",
    "print(\"Training samples: {}, Test samples: {}\".format(X_train_full.shape[0], X_test_data.shape[0]))\n",
    "print(\"Number of features: {}\".format(X_train_full.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training, validation, and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.2, random_state=7)\n",
    "\n",
    "print(\"Training: {}, Validation: {}, Test: {}\".format(\n",
    "    X_tr.shape[0], X_val.shape[0], X_test_data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shift the class labels by -1 so they start from 0\n",
    "y_tr = y_tr - 1\n",
    "y_val = y_val - 1\n",
    "y_test_shifted = y_test_data - 1\n",
    "\n",
    "num_classes = len(np.unique(y_train_full))\n",
    "num_features = X_tr.shape[1]\n",
    "print(\"Number of classes: {}\".format(num_classes))\n",
    "print(\"Number of features: {}\".format(num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batchsize and number of epochs\n",
    "batch_size = 32\n",
    "epochs = 50\n",
    "\n",
    "# Declare the sequential model and design your multi-layer perceptron\n",
    "model = models.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(num_features,)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile your model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train your model\n",
    "history = model.fit(X_tr, y_tr,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model summary\n",
    "model.summary()\n",
    "\n",
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.plot(history.history['loss'], label='Train Loss')\n",
    "ax1.plot(history.history['val_loss'], label='Val Loss')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "ax1.set_title('Loss')\n",
    "\n",
    "ax2.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "ax2.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "ax2.set_title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model on the test set\n",
    "test_loss, test_acc = model.evaluate(X_test_data, y_test_shifted, verbose=1)\n",
    "print(\"Test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model once you are satisfied with it\n",
    "model.save('mlp_har_model.keras')\n",
    "print(\"Model saved to mlp_har_model.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your saved model and test on the test data.\n",
    "loaded_model = keras.models.load_model('mlp_har_model.keras')\n",
    "\n",
    "test_loss, test_acc = loaded_model.evaluate(X_test_data, y_test_shifted, verbose=1)\n",
    "print(\"Loaded model test accuracy: {:.2f}%\".format(test_acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
